Fri Jan 4 16:08:12 EST 2008

did HAL malfunction?

--

I finally got around to watching, <i>2001: A Space Odyssey</i>, and while most of the movie's point escaped me, I did come out of it with at least one conclusion: I don't think the HAL 9000 computer malfunctioned, which makes their claims of the computer being "error free" true even though the computer seemed to have gone crazy.

First, I need to point out that I believe it is impossible for a computer like HAL to be created by humans.  That being said, I am going to suspend that belief so I can argue that HAL did not malfunction - given the technical achievement in the movie was a reality. 

When the trip to Jupiter sequence was introduced, it was hammered home that the computer was build to simulate a human brain.  It was also mentioned that some scientists believed that the computer merely mimicked the human brain, and I think that point was raised to bring up the question I am addressing right now.

Wikipedia defines psycopathy as, <i>classified by some as a personality disorder, characterized by amoral and antisocial behavior. It is a term derived from the Greek psyche (soul, breath hence mind) and pathos (to suffer), and was once used to denote any form of mental illness, often being confused with psychosis.</i>

While HAL was clearly acting in a psycopathic manner, it was still behaving as a psycopathic human. It can be said that a psycopathic human might be malfunctioning, but since no bounds were set on <i>which</i> human HAL was simulating, we can not say that it was failing to meet its design requirements.

Another interesting aspect to HAL is that it claims to have been "turned on" in Urbana, Illinois during the year 1993.  Coincidentally (or not very) Urbana is home to UUIC, which has a very strong computer science department.  It is home to NCSA, and is at the forefront of high performance computing research.  It is no stretch of the imagination to me that something like HAL would be build there (i.e., if I though this was even possible).  I digress.  

When HAL was "born," it recalls that its creator taught him a song (called, "Daisy", I believe).  This hints to the idea that HAL's intelligence was grown organically, and it was taught and raised as a human child might be.  This is not unlike the theme of many other AI movies - or research.  This implies that, compared to the complexity of its learned higher order cognative functions, the underlying technology and circuitry required to facilitate learning/searching/associating would not be overly complex.

This also implies that its intelligence was not explicitly designed, and it therefore became virtually impossible to know what it would do, how it would react, etc.  In otherwords, it would have been just as impossible to read its mind and predict what it would do as it is to do the same with a human mind (or neural net).  Furthermore, the only way to determine what its reaction would be to a given stimulus would be to observe it react in real time.  This means prediction would be impossible, and while statistical models of its behavior might have been able to be constructioned, it would be no more possible to predict its reaction based on a series of similar stimuli than it would to predict how a real person would behave to a series of similar stimuli.  Couple this with the fact that HAL was clearly a psycopath, and you get a system that you can not predict. This does beg the question, if you can't predict how a system would be have why claim it is error free?  However, it must be made clear - what system are you saying is error free?  The electronics and basic system software - or the orgically grown behavior model that drives much of its actions?

Who knows what happened to HAL in order to make it so paranoid and crazy.  One thing is pretty obvious - that it was acting out just like a human with a similar psycopathic condition would have given the situation.

Another thing I noticed, which would indicate an irrational core of being inside of HAL, was that instead of taking decisive action to protect itself from being dismantled it simply tried to convince Dave to not do so. 

If any part of HAL's behavior could have been considered an error, it would have been its lack of self preservation.  Then again, who is to say that a psycopath even feels this instinct if it doesn't feel other instincts or emotions?

There is no stong evidence to me that HAL did indeed malfunction.  To the contrary, I feel like the computing system worked as designed.  The emergence of its psycopathic condition can not be readily attributed to electronics or basic system programming. One can, however, easily show that HAL was accurately and convincingly simulating the actions and behavior of a truly psycopathic personality.  Perhaps the only human induced error of the system was not properly conditioning or raising HAL or its twin (mentioned midway through the sequence).

..interesting stuff, indeed.
